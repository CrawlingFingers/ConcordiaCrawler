<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
<title>COMP-598: Topics in Computer Science: Applied Machine Learning (Fall 2014)</title>
  </head>

<!-- Background white, links blue (unvisited), navy (visited), red (active) -->
 <BODY
  BGCOLOR="#FFFFFF"
  TEXT="#000000"
  LINK=blue
  VLINK=purple
  ALINK="#FF0000"
 >
<font=geneva,arial,helvetica>

<center><h2>Lectures for COMP-598: Topics in Computer Science: Applied Machine Learning</h2></center>
<center><h2>Fall 2014</h2></center>
</font>
<hr>

<p>
Some lecture notes will be linked to this web page, in PDF format.  The <a href="http://www.adobe.com/products/acrobat/readstep.html">reader for
PDF files</a> is available free from Adobe for UNIX, Apple Macintosh,
and Windows. 
<p>

<p>
<center><h3>Schedule</h3></center>

<center><table BORDER=3 CELLSPACING=0 CELLPADDING=2 WIDTH="95%" NOSAVE >
<tr NOSAVE>
<td WIDTH="3%"><center><b><i><font color="#006600">Lec.</font></i></b></center></td>
<td WIDTH="7%"><center><b><i><font color="#006600">Date</font></i></b></center></td>
<td WIDTH="40%"><center><b><i><font color="#006600">Topic</font></i></b></center></td>
<td WIDTH="20%"><center><b><i><font color="#006600">Lecture Material</font></i></b></center></td>
<td WIDTH="30%"><center><b><i><font color="#006600">Homeworks and Readings</font></i></b></center></td>
</tr>

<tr>
<td><center>1</center></td>
<td><center>Sep. 2</center></td>
<td><br><b>Introduction to machine learning.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/01Intro-publish.pdf">Slides</a></td>
<td>Read this <a href="http://dl.acm.org/citation.cfm?id=2347755">paper</a>.<br> Review basic notions of <a href="http://www.cs.mcgill.ca/~dprecup/courses/ML/Materials/linalg-review.pdf">algebra</a> and <a href="http://www.cs.mcgill.ca/~dprecup/courses/ML/Materials/prob-review.pdf">probabilities</a>.<br>Read Ch.1-2 of Bishop and Ch.1 of Hastie et al.</td>
</tr>

<tr>
<td><center>2</center></td>
<td><center>Sep.4</center></td>
<td><br><b>Linear regression.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/02LinearRegression-publish.pdf">Slides</a></td>
<td>Suggested readings:<br>
Ch.2 (Sec.2.1-2.4, 2.9) of Hastie et al.<br>
Ch.3 of Bishop (Sec.3.1-3.2).</td>
</tr>

<tr>
<td><center>3</center></td>
<td><center>Sep. 9</center></td>
<td><br><b>Linear regression.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/03LinearRegressionContd-publish.pdf">Slides</a><br>
<a href="http://www.cs.mcgill.ca/~jpineau/comp598/Projects/Project1_instructions.pdf">Mini-project #1</a> available.
</td>
<td>Suggested readings:<br>
Ch.3 (Sec.3.1-3.4, 3.9) of Hastie et al.<br>
Ch.3 of Bishop (Sec.3.1-3.2).
</td>
</tr>

<tr>
<td><center>4</center></td>
<td><center>Sep. 11</center></td>
<td><br><b>Linear classification.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/04LinearClassification-publish.pdf">Slides</a><br>
<td>Suggested readings:<br>
Ch.4 of Hastie et al.<br>
Ch.4 of Bishop (Sec.4.1-4.3).<br>
A <a href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf">paper</a> by Ng & Jordan (NIPS, 2001).
</td>
</tr>

<tr>
<td><center>5</center></td>
<td><center>Sep. 16</center></td>
<td><br><b>Naive Bayes.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/05NaiveBayes-publish.pdf">Slides</a></td>
<td>Suggested readings:<br>
Sec. 6.6.3 of Hastie et al.<br>
<a href="http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf">Ch.13</a> (Sec.13.1-13.4) of the book <a href="http://nlp.stanford.edu/IR-book/">Introduction to Information Retrieval</a>.</td>
</tr>

<tr>
<td><center>6</center></td>
<td><center>Sep. 18</center></td>
<td><br><b>Performance analysis and error estimation.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/06Evaluation-publish.pdf">Slides</a><br>
<td>Suggested readings:<br>
Ch.7 of Hastie et al.<br>
K. Wagstaff's (2012) <a href="http://www.wkiri.com/research/papers/wagstaff-MLmatters-12.pdf">paper</a></td>
</tr>

<tr>
<td><center>7</center></td>
<td><center>Sep. 23</center></td>
<td><br><b>Practical session with python and scikit-learn.</b></td>
<td>Mini-project #1 due.</td>
<td></td>
</tr>

<tr>
<td><center>8</center></td>
<td><center>Sep. 25</center></td>
<td><br><b>Decision trees</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/08DecisionTrees-publish.pdf">Slides</a><br>
<a href="http://www.cs.mcgill.ca/~jpineau/comp598/Projects/Project2_instructions.pdf">Mini-project #2</a> available.</td>
<td>Suggested readings:<br>
Sec.14.4 of Bishop.<br>
Sec.9.2 of Hastie et al.</td>
</tr>

<tr>
<td><center>9</center></td>
<td><center>Sep. 30</center></td>
<td><br><b>Instance-based learning</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/09InstanceLearning-publish.pdf">Slides</a></td>
<td>Suggested readings:<br>
Sec.2.5 of Bishop.<br>
Sec.13.3 of Hastie et al.</td>
</td>
</tr>

<tr>
<td><center>10</center></td>
<td><center>Oct. 2</center></td>
<td><br><b>Ensemble methods</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/10EnsembleLearning-publish.pdf">Slides</a><br>
<td>Suggested readings:<br>
Sec.8.7, Ch.10 of Hastie et al.<br>
Ch.14 of Bishop</td>
</tr>

<tr>
<td><center>11</center></td>
<td><center>Oct. 7</center></td>
<td><br><b>Ensemble learning (cont'd)</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/11EnsembleLearning2-publish.pdf">Slides</a><br>
<td></td>
</tr>

<tr>
<td><center>12</center></td>
<td><center>Oct. 9</center></td>
<td><br><b>Support vector machines</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/12SVM-publish.pdf">Slides</a></td>
<td>Suggested readings:<br>
Sec.7.1 of Bishop.<br>
Ch.12 (Sec.12.1-12.4) of Hastie et al.<br>
For background on convex optimization: see this <a href="http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">book</a> by S. Boyd and L. Vandenberghe</td>
</tr>

<tr>
<td><center>13</center></td>
<td><center>Oct. 14</center></td>
<td><br><b>Support vector machines (cont'd)</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/13SVM2-publish.pdf">Slides</a><br>
Mini-project #2 due.</td>
<td></td>
</tr>

<tr>
<td><center>14</center></td>
<td><center>Oct. 16</center></td>
<td><br><b>Neural networks</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/14NeuralNets-publish.pdf">Slides</a><br>
<a href="http://www.cs.mcgill.ca/~jpineau/comp598/Projects/Project3_instructions.pdf">Mini-project #3</a> available.</td>
<td>Suggested readings<br>
Ch.11 of Hastie et al.<br>
Sec.5.1-5.3 of Bishop</td>
</tr>

<tr>
<td><center>15</center></td>
<td><center>Oct. 21</center></td>
<td><br><b>Neural networks (cont'd)</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/15NeuralNets2-publish.pdf">Slides</a><br>
<td></td>
</tr>

<tr>
<td><center>16</center></td>
<td><center>Oct. 23</center></td>
<td><br><b>Feature construction and selection</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/16Features-publish.pdf">Slides</a><br>
<td></td>
</tr>

<tr>
<td><center>17</center></td>
<td><center>Oct. 28</center></td>
<td><br><b>Deep learning</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/17DeepLearning-publish.pdf">Slides</a><br>
See last slide for readings, tutorials and code suggestions.
<td></td>
</tr>

<tr>
<td><center>17B</center></td>
<td><center>Oct. 30</center></td>
<td><br><b>Problem solving session</b></td>
<td>See Discussion board on myCourses (Under topic "Midterme exam") for the practice questions. Solutions will be posted there in a few days.</td>
<td></td>
</tr>

<tr>
<td><center>18</center></td>
<td><center>Nov. 4</center></td>
<td><br><b>Unsupervised learning</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/18Unsupervised-publish.pdf">Slides</a><br>
Mini-project #3 due.</td>
<td></td>
</tr>

<tr>
<td><center>19</center></td>
<td><center>Nov. 6</center></td>
<td><br><b>Online / streaming data</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/19Online-publish.pdf">Slides</a><br>
<a href="http://www.cs.mcgill.ca/~jpineau/comp598/Projects/Project4_instructions.pdf">Final project</a> available.</td>
<td></td>
</tr>

<tr>
<td><center>20</center></td>
<td><center>Nov. 11</center></td>
<td><br><b>Semi-supervised learning</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/20Semisupervised-publish.pdf">Slides</a></td>
<td></td>
</tr>

<tr>
<td><center>21</center></td>
<td><center>Nov. 13</center></td>
<td><br><b>Parallelization for large-scale ML</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/21Parallelization-publish.pdf">Slides</a></td>
<td></td>
</tr>
				   
<tr>
<td><center>22</center></td>
<td><center>Nov. 18</center></td>
<td><br><b><FONT COLOR="FF0000">Midterm (confirmed date).</FONT></b></td>
<td></td>
<td></td>
</tr>

<tr>
<td><center>23</center></td>
<td><center>Nov. 20</center></td>
<td><br><b>Missing data.</b></td>
<td><a href="http://www.cs.mcgill.ca/~jpineau/comp598/Lectures/23Missingdata_publish.pdf">Slides</a></td>
<td></td>
</tr>

<tr>
<td><center>24</center></td>
<td><center>Nov. 25</center></td>
<td><br><b>Distributed Stochastic Convex Optimization<br>
Guest speaker:  Michael Rabbat</b></td>
<td></td>
<td></td>
</tr>

<tr>
<td><center>25</center></td>
<td><center>Nov. 27</center></td>
<td><br><b>Final project presentation session</b></td>
<td></td>
<td></td>
</tr>

<tr>
<td><center>26</center></td>
<td><center>Dec. 2</center></td>
<td><br><b>Wrap-up</b></td>
<td>Final project report due on Dec.8.</td>
<td></td>
</tr>


</table></center>


  </body>
</html>



